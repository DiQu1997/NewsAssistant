{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "def LLM_Query(system_prompt, user_query, temperature=0.9, top_p=1):\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "            ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    print(response)\n",
    "    response_content = response.choices[0][\"message\"][\"content\"]\n",
    "    query_tokens = response[\"usage\"][\"total_tokens\"]\n",
    "    return response_content, query_tokens\n",
    "\n",
    "newsdata_api = \"https://newsdata.io/api/1/news\"\n",
    "newsdata_archive_api = \"https://newsdata.io/api/1/archive\"\n",
    "\n",
    "categories = \"business,politics,technology\"\n",
    "newsdata_source = \"wsj,bloomberg\"\n",
    "full_content_source = \"usatoday\"#\"businessinsider_us,guardian,cnn,bbc\"\n",
    "\n",
    "def read_api_keys(key_file):\n",
    "    keys = None\n",
    "    with open(key_file) as f:\n",
    "        keys = json.load(f)\n",
    "    return keys\n",
    "\n",
    "def GetRequestEmbedding(customer_query):\n",
    "    response = openai.Embedding.create(\n",
    "        input=customer_query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "\"\"\"\n",
    "    News Info needed:\n",
    "    - Title\n",
    "    - Content(Description, depends on the api)\n",
    "    - URL\n",
    "    - Source\n",
    "    - Date\n",
    "\"\"\"\n",
    "\n",
    "# Read news from News IO source\n",
    "# TODO: Needs to refactor this function\n",
    "def read_news_io_api(api_key, api_endpoint, categories, domain, date_from):\n",
    "    url = api_endpoint + \"?apikey=\" + api_key + \"&category=\" + categories + \"&domain=\" + domain + \"&full_content=1\"\n",
    "    response = requests.get(url).json()\n",
    "    if response[\"status\"] != \"success\":\n",
    "        print(\"Failed to read news from News IO API\")\n",
    "        print(response)\n",
    "        return None\n",
    "    \n",
    "    nextPage = response[\"nextPage\"]\n",
    "    articles = []\n",
    "    for article in response[\"results\"]:\n",
    "        if datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\") > date_from:\n",
    "            articles.append(article)\n",
    "    \n",
    "    has_next_page = len(articles) > 0 and len(response[\"results\"]) < response[\"totalResults\"]\n",
    "    read_results = len(articles)\n",
    "    while has_next_page:\n",
    "        response = requests.get(url + \"&page=\" + str(nextPage)).json()\n",
    "        if response[\"status\"] != \"success\":\n",
    "            print(\"Failed to read news from News IO API\")\n",
    "            print(response)\n",
    "            break\n",
    "        nextPage = response[\"nextPage\"]\n",
    "        \n",
    "        page_articles = []\n",
    "        for article in response[\"results\"]:\n",
    "            if datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\") > date_from:\n",
    "                page_articles.append(article)\n",
    "            else:\n",
    "                print(datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        read_results += len(response[\"results\"])\n",
    "        has_next_page = len(page_articles) > 0 and read_results < response[\"totalResults\"]\n",
    "        articles.extend(page_articles)\n",
    "\n",
    "    # Transform the articles to required format\n",
    "    formalized_articles = []\n",
    "    for article in articles:\\\n",
    "        formalized_articles.append({\n",
    "            \"title\": article[\"title\"],\n",
    "            \"content\": article[\"description\"],\n",
    "            \"url\": article[\"link\"],\n",
    "            \"source\": article[\"source_id\"],\n",
    "            \"date\": datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    return formalized_articles, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = read_api_keys(os.environ[\"HOME\"] + \"/.api_keys.json\")\n",
    "news_data_api_key = api_keys[\"news_data_api\"]\n",
    "date = datetime.datetime.now() - datetime.timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles, response = read_news_io_api(news_data_api_key, newsdata_archive_api, categories, \"usa today\", date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_keys[\"openai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for article in news_articles:\n",
    "    embeddings.append(GetRequestEmbedding(article[\"title\"] + \" \" + article[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(news_articles)):\n",
    "    news_articles[i][\"date\"] = news_articles[i][\"date\"].strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.array(embeddings)\n",
    "np.save(\"news_embeddings.npy\", np_embeddings)\n",
    "json.dump(news_articles, open(\"news_articles.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.load(\"news_embeddings.npy\")\n",
    "news_articles = json.load(open(\"news_articles.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clustering for news articles\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "scores = []\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(np_embeddings)\n",
    "    scores.append(silhouette_score(np_embeddings, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "import json\n",
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "import argparse\n",
    "\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n",
    "grab_news_link_prompt = \"Grab all the news link and its title\"\n",
    "grab_news_content_prompt = \"Grab the full news content and its title, author\"\n",
    "\n",
    "def smart_scraper_graph(source, prompt, graph_config):\n",
    "    smart_scraper_graph = SmartScraperGraph(\n",
    "        prompt=prompt,\n",
    "        source=source,\n",
    "        config=graph_config\n",
    "    )\n",
    "\n",
    "    result = smart_scraper_graph.run()\n",
    "    return result\n",
    "\n",
    "def get_news_link(source):\n",
    "    return smart_scraper_graph(source, grab_news_link_prompt, graph_config)\n",
    "\n",
    "def get_news_content(links):\n",
    "    scraped_news = {}\n",
    "    for link in links:\n",
    "        result = smart_scraper_graph(link, grab_news_content_prompt, graph_config)\n",
    "        print(result)\n",
    "        scraped_news[result['title']] = {\n",
    "            'link': link,\n",
    "            'content': result['content'],\n",
    "            'author': result['author']   \n",
    "        }\n",
    "        \n",
    "    return scraped_news\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    api_keys_path = os.path.expanduser(\"~/.api_keys.json\")\n",
    "    try:\n",
    "        with open(api_keys_path, \"r\") as api_keys_file:\n",
    "            api_keys = json.load(api_keys_file)\n",
    "            OPENAI_API_KEY = api_keys.get(\"openai\")\n",
    "        \n",
    "            if not OPENAI_API_KEY:\n",
    "                print(\"警告：在 ~/.api_keys.json 文件中未找到 'openai' API 密钥。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：未找到文件 {api_keys_path}\")\n",
    "        os.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误：无法解析 {api_keys_path} 文件中的 JSON 数据\")\n",
    "        os.exit(1)\n",
    "        \n",
    "\n",
    "    graph_config = {\n",
    "        \"llm\": {\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "            \"model\": \"openai/gpt-4o-mini\",\n",
    "        },\n",
    "        \"verbose\": True,\n",
    "        \"headless\": False,\n",
    "    }\n",
    "\n",
    "    news_source = \"https://www.reuters.com/\"\n",
    "    news_links = get_news_link(news_source)\n",
    "    news_content = get_news_content(news_links)\n",
    "    print(news_content)\n",
    "    display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"下载原始数据_规则824584_2024-04-11_16-34-01-792106-20240411163530.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(data[\"Unnamed: 9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://www.reuters.com/markets/companies/TSLA.OQ/key-metrics/price-and-volume'\n",
    "page = requests.get(url)\n",
    "soup = bs(page.text, 'html.parser')\n",
    "\n",
    "# Locate the Table you wish to scrape\n",
    "table = soup.select_one('table.table__table__2px_A')\n",
    "\n",
    "# Locate the Keys and Value for each of the rows\n",
    "keys = [i.text for i in table.select('tr th') if i]\n",
    "values = [i.text for i in table.select('tr td') if i]\n",
    "\n",
    "# Convert the two lists into a dictionary for a neater output\n",
    "data = dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "with open(os.path.expanduser(\"~/Google-Drive/AI-Brain/database/content/Wed 1 Jan 2025 16.47 EST_Justin Caporale Donald Trump.txt\"), 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\"\"\"\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_or_opinion_classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",  \n",
    "    \"\"\"\n",
    "        You are a professional news editor. A news can be reporting news or publishing an opinion.\n",
    "        Verify the following content is news or opinion.\n",
    "        If it is reporting news, you need to return \"news\". \n",
    "        If it is publishing an opinion, you need to return \"opinion\".\n",
    "    \"\"\"),\n",
    "    (\"user\", \n",
    "    \"\"\"\n",
    "        The content is:\n",
    "        {content}\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "news_or_opinion_classifier = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "news_or_opinion_classifier.invoke(news_or_opinion_classifier_prompt.format(content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"You are a professional news editor. You are given an article content.\n",
    "From the news, extract the following information:\n",
    "What is the news about (key message)?\n",
    "Organization or People involved in the news and how are they involved or what happened to them\n",
    "What's the impact level of the news?\n",
    "What's the sentiment of the news?\n",
    "The return format should be a json with the following keys:\n",
    "- key_message: the key message of the news\n",
    "- entities: [\n",
    "        \"organization or people\": organization or people involved in the news,\n",
    "        \"how involved\": how are they involved or what happened to them\n",
    "] (if there is no organization or people involved, the list should be empty)\n",
    "- impact_level: the impact level of the news (low, medium, high)\n",
    "- sentiment: the sentiment of the news (positive, negative, neutral)\"\"\"),\n",
    "    (\"user\", \"The content is:\\n{content}\")\n",
    "])\n",
    "\n",
    "news_analyzer = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "result = news_analyzer.invoke(news_analysis_prompt.format(content=content))\n",
    "result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read result content as json\n",
    "json.loads(result.content[7:-3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
