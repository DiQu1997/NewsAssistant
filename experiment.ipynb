{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import openai\n",
    "import numpy as np\n",
    "\n",
    "from newsapi import NewsApiClient\n",
    "\n",
    "def LLM_Query(system_prompt, user_query, temperature=0.9, top_p=1):\n",
    "    response  = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "            ],\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    print(response)\n",
    "    response_content = response.choices[0][\"message\"][\"content\"]\n",
    "    query_tokens = response[\"usage\"][\"total_tokens\"]\n",
    "    return response_content, query_tokens\n",
    "\n",
    "newsdata_api = \"https://newsdata.io/api/1/news\"\n",
    "newsdata_archive_api = \"https://newsdata.io/api/1/archive\"\n",
    "\n",
    "categories = \"business,politics,technology\"\n",
    "newsdata_source = \"wsj,bloomberg\"\n",
    "full_content_source = \"usatoday\"#\"businessinsider_us,guardian,cnn,bbc\"\n",
    "\n",
    "def read_api_keys(key_file):\n",
    "    keys = None\n",
    "    with open(key_file) as f:\n",
    "        keys = json.load(f)\n",
    "    return keys\n",
    "\n",
    "def GetRequestEmbedding(customer_query):\n",
    "    response = openai.Embedding.create(\n",
    "        input=customer_query,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings\n",
    "\n",
    "\"\"\"\n",
    "    News Info needed:\n",
    "    - Title\n",
    "    - Content(Description, depends on the api)\n",
    "    - URL\n",
    "    - Source\n",
    "    - Date\n",
    "\"\"\"\n",
    "\n",
    "# Read news from News IO source\n",
    "# TODO: Needs to refactor this function\n",
    "def read_news_io_api(api_key, api_endpoint, categories, domain, date_from):\n",
    "    url = api_endpoint + \"?apikey=\" + api_key + \"&category=\" + categories + \"&domain=\" + domain + \"&full_content=1\"\n",
    "    response = requests.get(url).json()\n",
    "    if response[\"status\"] != \"success\":\n",
    "        print(\"Failed to read news from News IO API\")\n",
    "        print(response)\n",
    "        return None\n",
    "    \n",
    "    nextPage = response[\"nextPage\"]\n",
    "    articles = []\n",
    "    for article in response[\"results\"]:\n",
    "        if datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\") > date_from:\n",
    "            articles.append(article)\n",
    "    \n",
    "    has_next_page = len(articles) > 0 and len(response[\"results\"]) < response[\"totalResults\"]\n",
    "    read_results = len(articles)\n",
    "    while has_next_page:\n",
    "        response = requests.get(url + \"&page=\" + str(nextPage)).json()\n",
    "        if response[\"status\"] != \"success\":\n",
    "            print(\"Failed to read news from News IO API\")\n",
    "            print(response)\n",
    "            break\n",
    "        nextPage = response[\"nextPage\"]\n",
    "        \n",
    "        page_articles = []\n",
    "        for article in response[\"results\"]:\n",
    "            if datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\") > date_from:\n",
    "                page_articles.append(article)\n",
    "            else:\n",
    "                print(datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "        read_results += len(response[\"results\"])\n",
    "        has_next_page = len(page_articles) > 0 and read_results < response[\"totalResults\"]\n",
    "        articles.extend(page_articles)\n",
    "\n",
    "    # Transform the articles to required format\n",
    "    formalized_articles = []\n",
    "    for article in articles:\n",
    "        formalized_articles.append({\n",
    "            \"title\": article[\"title\"],\n",
    "            \"content\": article[\"description\"],\n",
    "            \"url\": article[\"link\"],\n",
    "            \"source\": article[\"source_id\"],\n",
    "            \"date\": datetime.datetime.strptime(article[\"pubDate\"], \"%Y-%m-%d %H:%M:%S\")\n",
    "        })\n",
    "    return formalized_articles, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_keys = read_api_keys(os.environ[\"HOME\"] + \"/.api_keys.json\")\n",
    "news_data_api_key = api_keys[\"news_data_api\"]\n",
    "date = datetime.datetime.now() - datetime.timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles, response = read_news_io_api(news_data_api_key, newsdata_archive_api, categories, \"usa today\", date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = api_keys[\"openai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for article in news_articles:\n",
    "    embeddings.append(GetRequestEmbedding(article[\"title\"] + \" \" + article[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(news_articles)):\n",
    "    news_articles[i][\"date\"] = news_articles[i][\"date\"].strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.array(embeddings)\n",
    "np.save(\"news_embeddings.npy\", np_embeddings)\n",
    "json.dump(news_articles, open(\"news_articles.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_embeddings = np.load(\"news_embeddings.npy\")\n",
    "news_articles = json.load(open(\"news_articles.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clustering for news articles\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "scores = []\n",
    "for i in range(2, 100):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(np_embeddings)\n",
    "    scores.append(silhouette_score(np_embeddings, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import os\n",
    "import json\n",
    "from pyvirtualdisplay import Display\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from scrapegraphai.graphs import SmartScraperGraph\n",
    "import argparse\n",
    "\n",
    "os.environ['PYVIRTUALDISPLAY_DISPLAYFD'] = '0'\n",
    "grab_news_link_prompt = \"Grab all the news link and its title\"\n",
    "grab_news_content_prompt = \"Grab the full news content and its title, author\"\n",
    "\n",
    "def smart_scraper_graph(source, prompt, graph_config):\n",
    "    smart_scraper_graph = SmartScraperGraph(\n",
    "        prompt=prompt,\n",
    "        source=source,\n",
    "        config=graph_config\n",
    "    )\n",
    "\n",
    "    result = smart_scraper_graph.run()\n",
    "    return result\n",
    "\n",
    "def get_news_link(source):\n",
    "    return smart_scraper_graph(source, grab_news_link_prompt, graph_config)\n",
    "\n",
    "def get_news_content(links):\n",
    "    scraped_news = {}\n",
    "    for link in links:\n",
    "        result = smart_scraper_graph(link, grab_news_content_prompt, graph_config)\n",
    "        print(result)\n",
    "        scraped_news[result['title']] = {\n",
    "            'link': link,\n",
    "            'content': result['content'],\n",
    "            'author': result['author']   \n",
    "        }\n",
    "        \n",
    "    return scraped_news\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()\n",
    "\n",
    "    api_keys_path = os.path.expanduser(\"~/.api_keys.json\")\n",
    "    try:\n",
    "        with open(api_keys_path, \"r\") as api_keys_file:\n",
    "            api_keys = json.load(api_keys_file)\n",
    "            OPENAI_API_KEY = api_keys.get(\"openai\")\n",
    "        \n",
    "            if not OPENAI_API_KEY:\n",
    "                print(\"警告：在 ~/.api_keys.json 文件中未找到 'openai' API 密钥。\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误：未找到文件 {api_keys_path}\")\n",
    "        os.exit(1)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"错误：无法解析 {api_keys_path} 文件中的 JSON 数据\")\n",
    "        os.exit(1)\n",
    "        \n",
    "\n",
    "    graph_config = {\n",
    "        \"llm\": {\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "            \"model\": \"openai/gpt-4o-mini\",\n",
    "        },\n",
    "        \"verbose\": True,\n",
    "        \"headless\": False,\n",
    "    }\n",
    "\n",
    "    news_source = \"https://www.reuters.com/\"\n",
    "    news_links = get_news_link(news_source)\n",
    "    news_content = get_news_content(news_links)\n",
    "    print(news_content)\n",
    "    display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"下载原始数据_规则824584_2024-04-11_16-34-01-792106-20240411163530.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(data[\"Unnamed: 9\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'https://www.reuters.com/markets/companies/TSLA.OQ/key-metrics/price-and-volume'\n",
    "page = requests.get(url)\n",
    "soup = bs(page.text, 'html.parser')\n",
    "\n",
    "# Locate the Table you wish to scrape\n",
    "table = soup.select_one('table.table__table__2px_A')\n",
    "\n",
    "# Locate the Keys and Value for each of the rows\n",
    "keys = [i.text for i in table.select('tr th') if i]\n",
    "values = [i.text for i in table.select('tr td') if i]\n",
    "\n",
    "# Convert the two lists into a dictionary for a neater output\n",
    "data = dict(zip(keys,values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "import os\n",
    "with open(os.path.expanduser(\"~/Google-Drive/AI-Brain/database/content/Wed 1 Jan 2025 16.47 EST_Justin Caporale Donald Trump.txt\"), 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_or_opinion_classifier_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",  \n",
    "    \"\"\"\n",
    "        You are a professional news editor. A news can be reporting news or publishing an opinion.\n",
    "        Verify the following content is news or opinion.\n",
    "        If it is reporting news, you need to return \"news\". \n",
    "        If it is publishing an opinion, you need to return \"opinion\".\n",
    "    \"\"\"),\n",
    "    (\"user\", \n",
    "    \"\"\"\n",
    "        The content is:\n",
    "        {content}\n",
    "    \"\"\"),\n",
    "])\n",
    "\n",
    "news_or_opinion_classifier = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "news_or_opinion_classifier.invoke(news_or_opinion_classifier_prompt.format(content=content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\" You are a professional news editor. You are given an article content.\n",
    "        From the news, extract the following information:\n",
    "        What is the news about (key message)?\n",
    "        Organization or People involved in the news and how are they involved or what happened to them\n",
    "        What's the impact level of the news?\n",
    "        What's the sentiment of the news?\n",
    "        The return format should be json format without extra type indicator with the following keys:\n",
    "        - key_message: the key message of the news\n",
    "        - what happened: what happened at what time\n",
    "        - entities: [\n",
    "                \"organization or people\": organization or people involved in the news, if the entity has aliases here, don't put it here, use the direct entity name from the article, \n",
    "                                          like IMF or International Monetary Fund, you can use either of them but not International Monetary Fund(IMF), the aliases should be shown in entity_aliases\n",
    "                \"how involved\": how are they involved or what happened to them\n",
    "        ] (if there is no organization or people involved, the list should be empty)\n",
    "        - entity_aliases: [\n",
    "                \"entity_name\": entity name,\n",
    "                \"aliases\": list of entity aliases that appear in the news, For example, if the entity is \"Apple\", the aliases could be \"AAPL\", \"Apple Inc.\", \"Apple Computer, Inc.\", etc.\n",
    "        ]\n",
    "        - impact_level: the impact level of the news (low, medium, high)\n",
    "        - sentiment: the sentiment of the news (positive, negative, neutral)\n",
    "    \"\"\"),\n",
    "    (\"user\", \"The content is:\\n{content}\")\n",
    "])\n",
    "\n",
    "news_analyzer = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "result = news_analyzer.invoke(news_analysis_prompt.format(content=content))\n",
    "result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opinion_analysis_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\" You are a professional news editor. You are given an article content. The article is an opinion to an event or an entity or some current situations over some place or the entire world.\n",
    "        From the opinion, extract the following information:\n",
    "        - What is the opinion about?\n",
    "        - Who is the opinion for?\n",
    "        - Who is the opinion against?\n",
    "        - A short summary of the opinion\n",
    "        - The logic / analysis that support the opinion\n",
    "        The return format should be json format without extra type indicator with the following keys:\n",
    "        - opinion_about: what is the opinion about\n",
    "        - opinion_for: who is the opinion for\n",
    "        - opinion_against: who is the opinion against\n",
    "        - opinion_summary: a short summary of the opinion\n",
    "        - supporting_reason: the evidence, analysis or logic that support the opinion\n",
    "    \"\"\"),\n",
    "    (\"user\", \"The content is:\\n{content}\")\n",
    "])\n",
    "\n",
    "opinion_analyzer = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "opinion_result = opinion_analyzer.invoke(opinion_analysis_prompt.format(content=content))\n",
    "opinion_result.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def normalize_datetime(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Handle 'YYYY-MM-DD' format\n",
    "        if len(date_str) == 10 and date_str.count('-') == 2:\n",
    "            return datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        \n",
    "        # Handle both 'Day DD Mon YYYY HH.MM EST' and 'Day DD Mon YYYY HH.MM' formats\n",
    "        elif any(day in date_str for day in ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']):\n",
    "            # Remove 'EST' if present and clean up the string\n",
    "            date_str = date_str.replace('EST', '').strip()\n",
    "            return datetime.strptime(date_str, '%a %d %b %Y %H.%M')\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unhandled date format: {date_str}\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing date '{date_str}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read result content as json\n",
    "import json\n",
    "json.loads(result.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SQLite database\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "conn = sqlite3.connect(os.path.expanduser(\"~/Google-Drive/AI-Brain/database/news_data.db\"))  # Using the same db file as NewsCollector.py\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Query the database\n",
    "cursor.execute(\"SELECT * FROM news_articles\")  # Using the table name from NewsCollector.py\n",
    "result = cursor.fetchall()\n",
    "print(result)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM news_articles\", conn)\n",
    "df[\"created_time\"] = df[\"created_time\"].apply(normalize_datetime)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all news content from files\n",
    "import os\n",
    "\n",
    "def load_news_content(file_path):\n",
    "    full_path = os.path.expanduser(f\"~/Google-Drive/AI-Brain/database/{file_path}\")\n",
    "    try:\n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {full_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Add content column to dataframe\n",
    "df['content'] = df['file_path'].apply(load_news_content)\n",
    "\n",
    "# Remove rows where content failed to load\n",
    "df = df.dropna(subset=['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_coref_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a professional news editor. You are given a list of known entities and a new entity. Please identify if the new entity is the same as one of the known entities. If it is, return the known entity name. If it is not, return a single world None. No extra output allowed\"\"\"),\n",
    "    (\"user\", \"The known entities are:\\n{known_entities}\\nThe new entity is:\\n{new_entity}\")\n",
    "])\n",
    "entity_coref_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher, get_close_matches\n",
    "from fuzzywuzzy import fuzz  # pip install fuzzywuzzy\n",
    "from Levenshtein import ratio\n",
    "\n",
    "def smart_entity_matcher(str1, str2, threshold=90):\n",
    "    \"\"\"\n",
    "    Combines multiple matching approaches for better entity matching\n",
    "    Returns: (bool, float) - (is_match, confidence_score)\n",
    "    \"\"\"\n",
    "    # Get scores from different methods\n",
    "    fuzzy_token_score = fuzz.token_set_ratio(str1, str2)\n",
    "    fuzzy_partial_score = fuzz.partial_ratio(str1, str2)\n",
    "    levenshtein_score = ratio(str1, str2) * 100  # Convert to percentage\n",
    "    \n",
    "    # Weight the scores (adjust weights based on your needs)\n",
    "    weighted_score = (\n",
    "        fuzzy_token_score * 0.5 +    # Token matching is most important\n",
    "        fuzzy_partial_score * 0.3 +   # Partial matching helps with abbreviations\n",
    "        levenshtein_score * 0.2       # Levenshtein helps catch typos\n",
    "    )\n",
    "    \n",
    "    return weighted_score >= threshold, weighted_score\n",
    "    \n",
    "# check if an entity can match one of the known entities\n",
    "def check_entity_match(entity, known_entities_mapping, threshold=90):\n",
    "    largest_score = 0\n",
    "    largest_score_entity = None\n",
    "    for known_entity, known_entity_aliases in known_entities_mapping.items():\n",
    "        for known_entity_alias in known_entity_aliases:\n",
    "            above_threshold, score = smart_entity_matcher(entity, known_entity_alias, threshold)\n",
    "            if above_threshold and score > largest_score:\n",
    "                largest_score = score\n",
    "                largest_score_entity = known_entity\n",
    "    return largest_score_entity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the news\n",
    "article_analysis_result = []\n",
    "entity_aliases_mapping = {} # entity_name -> list of entity aliases\n",
    "all_entities = set()\n",
    "for i, news in enumerate(df.iterrows()):\n",
    "    if i > 100:\n",
    "        break\n",
    "    title = news[1][\"title\"]\n",
    "    content = news[1][\"content\"]\n",
    "    print(f\"Processing news {i} of {len(df)}, title: {title}\")\n",
    "    news_or_opinion = news_or_opinion_classifier.invoke(news_or_opinion_classifier_prompt.format(content=content))\n",
    "    result = news_or_opinion.content\n",
    "    if result == \"opinion\":\n",
    "        succeed = False\n",
    "        for _ in range(0, 3):   \n",
    "            try:\n",
    "                opinion_result = opinion_analyzer.invoke(opinion_analysis_prompt.format(content=content))\n",
    "                opinion_result_json = json.loads(opinion_result.content)\n",
    "                opinion_result_json[\"type\"] = \"opinion\"\n",
    "                article_analysis_result.append(opinion_result_json)\n",
    "                succeed = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing opinion: {e}\")\n",
    "        if not succeed:\n",
    "            print(f\"Failed to analyze opinion: {content}\")\n",
    "            article_analysis_result.append({\"type\": \"opinion\", \"error\": str(e)})\n",
    "    else:\n",
    "        succeed = False\n",
    "        for _ in range(0, 3):\n",
    "            try:\n",
    "                news_result = news_analyzer.invoke(news_analysis_prompt.format(content=content))\n",
    "                news_result_json = json.loads(news_result.content)\n",
    "                news_result_json[\"type\"] = \"news\"\n",
    "                article_analysis_result.append(news_result_json)\n",
    "                succeed = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing news: {e}\")\n",
    "        if not succeed:\n",
    "            print(f\"Failed to analyze news: {content}\")\n",
    "            article_analysis_result.append({\"type\": \"news\", \"error\": str(e)})\n",
    "        else:\n",
    "            for entity in news_result_json[\"entity_aliases\"]:\n",
    "                if entity[\"entity_name\"] not in all_entities:\n",
    "                    match_entity = check_entity_match(entity[\"entity_name\"], entity_aliases_mapping)\n",
    "                    if match_entity is not None:\n",
    "                        entity_aliases_mapping[match_entity].append(entity[\"entity_name\"])\n",
    "                    else:\n",
    "                        match_entity = entity[\"entity_name\"]\n",
    "                        all_entities.add(match_entity)   \n",
    "                        entity_aliases_mapping[match_entity] = [entity[\"entity_name\"]]\n",
    "                    for alias in entity[\"aliases\"]:\n",
    "                        if alias not in all_entities:\n",
    "                            all_entities.add(alias)\n",
    "                            entity_aliases_mapping[match_entity].append(alias)\n",
    "            for entity in news_result_json[\"entities\"]:\n",
    "                if entity[\"organization or people\"] not in all_entities:\n",
    "                    match_entity = check_entity_match(entity[\"organization or people\"], entity_aliases_mapping)\n",
    "                    if match_entity is not None:\n",
    "                        entity_aliases_mapping[match_entity].append(entity[\"organization or people\"])\n",
    "                    else:\n",
    "                        all_entities.add(entity[\"organization or people\"])\n",
    "                        entity_aliases_mapping[entity[\"organization or people\"]] = [entity[\"organization or people\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_aliases_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(entity, entity_aliases_mapping[entity]) for entity in entity_aliases_mapping if len(entity_aliases_mapping[entity]) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_entity = smart_entity_matcher('New York City Mayor Eric Adams',\n",
    "   'Eric Adams')\n",
    "match_entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the entities and identify the same entity with different name\n",
    "entities = []\n",
    "entities_mapping = {} # key: entity name, value: common entity name in full\n",
    "\n",
    "for article in article_analysis_result:\n",
    "    if article[\"type\"] == \"news\":\n",
    "        for entity in article[\"entities\"]:\n",
    "            if entity[\"organization or people\"] not in entities:\n",
    "                entities.append(entity[\"organization or people\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_entities = list(set(entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_result.content.replace(\"```json\", \"\").replace(\"```\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry_LLM_request_json_format(ChatOpenAI_model, prompt, retry_count=3):\n",
    "    for _ in range(0, retry_count):\n",
    "        try:    \n",
    "            result = ChatOpenAI_model.invoke(prompt)\n",
    "            if result.content is not None:\n",
    "                # Clean up the content first\n",
    "                content = result.content.strip()\n",
    "                # Remove any markdown code block markers\n",
    "                content = content.replace(\"```json\", \"\").replace(\"```\", \"\")\n",
    "                \n",
    "                # Split into individual JSON objects and parse each one\n",
    "                json_result = json.loads(content)\n",
    "                return json_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in retry_LLM_request_json_format: {e}\\nContent:\\n{result.content}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_compare_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "    \"\"\"\n",
    "        You are an information editor tasked with comparing two sets of articles to determine if they are discussing the same event or story. \n",
    "        To define the same event or story, imagine you are a news editor. And you have a special section for each event, and the event could evolve among time, you need to find if the news is related to the event\n",
    "        Your job is to analyze the content and provide a relatedness score on a scale of 1-10, where 1 means completely unrelated and 10 means they are definitely about the same event or story.\n",
    "    \"\"\"),\n",
    "    (\"user\", \n",
    "    \"\"\"\n",
    "        Here is the first article:\n",
    "        <article1>\n",
    "        {Event}\n",
    "        </article1>\n",
    "\n",
    "        Here is the second article or set of information:\n",
    "        <article2>\n",
    "        {news}\n",
    "        </article2>\n",
    "\n",
    "        Please carefully read and analyze both pieces of content. Look for key elements such as:\n",
    "        - Similar events, incidents, or topics\n",
    "        - Matching names of people, places, or organizations\n",
    "        - Corresponding dates or time frames\n",
    "        - Overlapping details or facts\n",
    "\n",
    "        After your analysis, determine how related the two pieces of content are. Consider the following guidelines for scoring:\n",
    "        - 1-3: Mostly or completely unrelated topics\n",
    "        - 4-6: Some overlapping themes or tangentially related topics\n",
    "        - 7-10: Highly likely or certainly about the same event or story\n",
    "\n",
    "        Format your response as a single number from 1-10, no other text.\n",
    "\n",
    "    \"\"\"),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "maximum_news_count = 10\n",
    "class Event:\n",
    "    def __init__(self, event_id=0):\n",
    "        self.event_id = event_id\n",
    "        self.event_name = \"\"\n",
    "        self.event_description = \"\"\n",
    "        self.raw_news_array = []\n",
    "        self.related_scores = []\n",
    "        self.event_news_per_date = {} # date: [index of news in raw_news_array]\n",
    "        # self.event_news_daily_summary = {} # date: summary of news in the day\n",
    "        self.total_news_count = 0\n",
    "\n",
    "    def _compare_between_events_and_news(self, news_to_compare, event_to_compare):\n",
    "        model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        result = model.invoke(news_compare_prompt.format(Event=event_to_compare, news=news_to_compare))\n",
    "        return result.content\n",
    "\n",
    "    def _compare_by_random_news(self, news_to_compare, news_count=3):\n",
    "        random_news = random.choices(self.raw_news_array, k=news_count)\n",
    "        random_news_content = \"\"\n",
    "        for news in random_news[:maximum_news_count]:\n",
    "            random_news_content += news\n",
    "\n",
    "        for _ in range(0, 3):   \n",
    "            try:\n",
    "                score = self._compare_between_events_and_news(random_news_content, news_to_compare)\n",
    "                score = int(score)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing news: {e}, error response: {score}\")\n",
    "                score = 0\n",
    "        return score\n",
    "\n",
    "    def _compare_by_latest_date(self, news_to_compare, date_count=3):\n",
    "        dates = list(self.event_news_per_date.keys()) \n",
    "        dates.sort()\n",
    "        chosen_dates = dates[-date_count:]\n",
    "        chosen_news = []\n",
    "        for date in chosen_dates:\n",
    "            chosen_news.append(self.event_news_per_date[date])\n",
    "        chosen_news_content = \"\"\n",
    "        for news in chosen_news[:maximum_news_count]:\n",
    "            chosen_news_content += self.raw_news_array[news]\n",
    "\n",
    "        for _ in range(0, 3):\n",
    "            try:\n",
    "                score = self._compare_between_events_and_news(chosen_news_content, news_to_compare)\n",
    "                score = int(score)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing news: {e}, error response: {score}\")\n",
    "                score = 0\n",
    "        return score\n",
    "\n",
    "    def _compare_by_event_description(self, event_description):\n",
    "        for _ in range(0, 3):\n",
    "            try:\n",
    "                score = self._compare_between_events_and_news(self.event_description, event_description)\n",
    "                score = int(score)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error comparing news: {e}, error response: {score}\")\n",
    "                score = 0\n",
    "        return score\n",
    "        \n",
    "    def add_news_to_store(self, news, score=0):\n",
    "        self.raw_news_array.append(news)\n",
    "        self.total_news_count += 1\n",
    "        self.event_news_per_date[self.total_news_count] = news\n",
    "        self.related_scores.append(score)\n",
    "        \n",
    "    def check_if_news_is_related(self, news, compare_strategy=\"random\"):\n",
    "        if compare_strategy == \"random\":\n",
    "            score = self._compare_by_random_news(news)\n",
    "        elif compare_strategy == \"latest_date\":\n",
    "            score = self._compare_by_latest_date(news)\n",
    "        elif compare_strategy == \"event_description\":\n",
    "            score = self._compare_by_event_description(news)\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "News_clusters = {}\n",
    "\n",
    "df.sort_values(by=\"created_time\", ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the news and start the clustering.\n",
    "# if the News_cluster is empty, start a new cluster.\n",
    "# if the News_cluster is not empty, compare the news with the news in the cluster.\n",
    "# if the news is related to the news in the cluster, add the news to the cluster.\n",
    "# if the news is not related to the news in the cluster, start a new cluster.\n",
    "\n",
    "# Note / TODO: The news is too widely distributed, we need two things:\n",
    "# 1. Need some fast recall mechanism to find the filter all the news clusters: name entities for example \n",
    "# 2. Hierarchical clustering. \n",
    "# 3. Garbage cluster that collect those single meaning less news which has no way to find other related news\n",
    "News_clusters = {}\n",
    "\n",
    "highest_cluster_id = 0\n",
    "for i, news in enumerate(df.iterrows()):\n",
    "    if (i > 100):\n",
    "        break\n",
    "\n",
    "    title = news[1][\"title\"]\n",
    "    content = news[1][\"content\"]\n",
    "    created_time = news[1][\"created_time\"]\n",
    "    print(f\"Processing news {i} of {len(df)}, title: {title}\")\n",
    "\n",
    "    if News_clusters == {}:\n",
    "        picked_cluster = Event(highest_cluster_id)\n",
    "        highest_cluster_id += 1\n",
    "        News_clusters[picked_cluster.event_id] = picked_cluster\n",
    "        max_score = 10\n",
    "    else:\n",
    "        cluster_score = {}\n",
    "        max_score = 0\n",
    "        for cluster_id, cluster in News_clusters.items():\n",
    "            score = cluster.check_if_news_is_related(content)\n",
    "            print(f\"score: {score}, cluster_id: {cluster_id}\")\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                max_score_cluster_id = cluster_id\n",
    "        if (max_score >= 7):\n",
    "            picked_cluster = News_clusters[max_score_cluster_id]\n",
    "        else:\n",
    "            picked_cluster = Event(highest_cluster_id)\n",
    "            highest_cluster_id += 1\n",
    "            News_clusters[picked_cluster.event_id] = picked_cluster\n",
    "            max_score = 10\n",
    "    picked_cluster.add_news_to_store(content, max_score)\n",
    "    print(\"total cluster count: \", len(News_clusters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and content for embedding\n",
    "df['text_for_embedding'] = df['title'] + \" \" + df['content']\n",
    "\n",
    "# Get embeddings using LangChain with OpenAI embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Get embeddings for all documents\n",
    "print(\"Generating embeddings...\")\n",
    "embedded_docs = []\n",
    "for text in df['text_for_embedding']:\n",
    "    embedded_docs.append(embeddings.embed_query(text))\n",
    "\n",
    "# Convert to numpy array\n",
    "embedded_docs = np.array(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "n_clusters = 15  # Adjust number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(embedded_docs)\n",
    "\n",
    "# Display results with cluster assignments\n",
    "print(f\"\\nDocuments clustered into {n_clusters} groups:\")\n",
    "df[['title', 'cluster', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster'] == 1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
